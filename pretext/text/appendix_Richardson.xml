<?xml version="1.0" encoding="UTF-8" ?>
<!-- Copyright 2018-2020 Joel Feldman, Andrew Rechnitzer and Elyse Yeager -->
<!-- This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License-->
<!-- https://creativecommons.org/licenses/by-nc-sa/4.0 -->

<appendix  xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="app_Richardson_Romberg">
  <title>More About Numerical Integration</title>

<introduction>
<p>

</p>
</introduction>

<section xml:id="ap_Richardson">
  <title>Richardson Extrapolation</title>
<p>
There are many approximation procedures in which one first picks a
step size <m>h</m> and then generates an approximation <m>A(h)</m> to some
desired quantity <m>\cA</m>. For example, <m>\cA</m> might be the value of
some integral <m>\int_a^b f(x)\,\dee{x}</m>. For the trapezoidal rule with <m>n</m> steps,
<m>\De x=\tfrac{b-a}{n}</m> plays the role of the step size.  Often the order of the error generated by the procedure is known. This means
<me>
\cA=A(h)+Kh^k+K_1h^{k+1}+K_2h^{k+2}+\ \cdots
\tag{E1}</me>
with <m>k</m> being some known constant, called the order of the error, and <m>K,\ K_1,\ K_2,\
\cdots</m> being some other
(usually unknown) constants. If <m>A(h)</m> is the approximation to <m>\cA=\int_a^b f(x)\,\dee{x}</m> produced by the trapezoidal rule with <m>\De x=h</m>, then <m>k=2</m>. If Simpson's rule
is used, <m>k=4</m>.
</p>

<p>
Let's first suppose that <m>h</m> is small enough that the terms
<m>K'h^{k+1}+K''h^{k+2}+\ \cdots</m> in (E1) are small
enough<fn>Typically, we don't have access to, and don't care about, the exact error. We only care about its order of magnitude. So if <m>h</m> is small enough that <m>K_1h^{k+1}+K_2h^{k+2}+\ \cdots</m> is a factor of at least, for example, one hundred smaller than <m>Kh^k</m>, then dropping <m>K_1h^{k+1}+K_2h^{k+2}+\ \cdots</m> would not bother us at all.</fn> that dropping them has essentially no impact.
This would give
<me>
\cA=A(h)+Kh^k
\tag{E2}</me>
Imagine that we know <m>k</m>, but that we do not know <m>A</m> or <m>K</m>,
and think of (E2) as an equation that
the unknowns <m>\cA</m> and <m>K</m> have to solve. It may look like we have
one equation in the two unknowns <m>K</m>, <m>\cA</m>, but that is <em>not</em>
the case.
The reason is that (E2) is (essentially) true for
all (sufficiently small) choices of <m>h</m>.
If we pick some <m>h</m>, say <m>h_1</m>, and use the algorithm to determine <m>A(h_1)</m>
then (E2), with <m>h</m> replaced by <m>h_1</m>, gives
one equation in the two unknowns <m>\cA</m> and <m>K</m>, and if we then
pick some
different <m>h</m>, say <m>h_2</m>, and use the algorithm a second time
to determine <m>A(h_2)</m>
then (E2), with <m>h</m> replaced by <m>h_2</m>, gives a
second equation in the two unknowns <m>\cA</m> and <m>K</m>. The two equations will
then determine both <m>\cA</m> and <m>K</m>.
</p>

<p>
To be more concrete, suppose that we have picked some specific value of <m>h</m>, and
have chosen <m>h_1=h</m> and <m>h_2=\tfrac{h}{2}</m>, and that we have evaluated
<m>A(h)</m> and <m>A(h/2)</m>. Then the two equations are
<md>
<mrow>
\cA&amp;=A(h)+Kh^k \tag{E3a}
</mrow><mrow>
\cA&amp;=A(h/2)+K\big(\tfrac{h}{2}\big)^k \tag{E3b}
</mrow>
</md>
It is now easy to solve for both <m>K</m> and <m>\cA</m>. To get <m>K</m>, just subtract
(E3b) from (E3a).
<md>
<mrow>
(\textrm{E3a})-(\textrm{E3b}):&amp;\quad
0=A(h)-A(h/2) +\big(1-\tfrac{1}{2^k}\big)Kh^k
</mrow><mrow>
\implies&amp;\quad
K=\frac{A(h/2)-A(h)}{[1-2^{-k}]h^k}
\tag{E4a}
</mrow>
</md>
To get <m>\cA</m> multiply (E3b) by <m>2^k</m> and then
subtract (E3a).
<md>
<mrow>
2^k(\textrm{E3b})-(\textrm{E3a}):&amp;\quad
[2^k-1]\cA=2^kA(h/2)-A(h)
</mrow><mrow>
\implies&amp;\quad
\cA=\frac{2^kA(h/2)-A(h)}{2^k-1}
\tag{E4b}
</mrow>
</md>
The generation of a <q>new improved</q> approximation
for <m>\cA</m> from two <m>A(h)</m>'s with different values of <m>h</m> is called 
Richardson<fn>Richardson extrapolation
was introduced by the Englishman Lewis Fry Richardson
(1881--1953) in 1911.</fn>
Extrapolation.  Here is a summary
</p>
<fact xml:id="eq_Richardson"><title>Richardson extrapolation</title>
<statement><p>
Let <m>A(h)</m> be a step size <m>h</m> approximation to <m>\cA</m>. If
<me>
\cA=A(h)+K h^k
</me>
then
<me>
K=\frac{A(h/2)-A(h)}{[1-2^{-k}]h^k}\qquad
\cA=\frac{2^kA(h/2)-A(h)}{2^k-1}
</me>
</p>
</statement>
</fact>

<p>
This works very well since, by computing <m>A(h)</m> for two different <m>h</m>'s, 
we can remove the biggest error term in (E1), and so get a much more precise approximation to <m>\cA</m> for little additional work.
</p>


<example xml:id="eg_Richard"><title>Richardson extrapolation with the trapezoidal rule</title>
<p>
Applying the trapezoidal rule (<xref ref="eq_TRPrule"/>) to the integral <m>\cA=\int_0^1\frac{4}{1+x^2}\dee{x}</m>
with step sizes <m>\frac{1}{8}</m> and <m>\frac{1}{16}</m>
(i.e. with <m>n=8</m> and <m>n=16</m>) gives, with <m>h=\frac{1}{8}</m>,
<me>
A(h)=3.1389884945\qquad
A(h/2)=3.1409416120
</me>
So (E4b), with <m>k=2</m>, gives us
the <q>new improved</q> approximation
<me>
\frac{2^2\times 3.1409416120 -3.1389884945}{2^2-1}=3.1415926512
</me>
We saw in Example<nbsp/><xref ref="eg_MidpointB"/> that <m>\int_0^1\frac{4}{1+x^2}\dee{x}=\pi</m>,
so this new approximation really is <q>improved</q>:
<ul>
<li>
 <m>A(1/8)</m> agrees with <m>\pi</m> to two decimal places,
</li><li>
<m>A(1/16)</m>  agrees with <m>\pi</m> to three decimal places
            and
</li><li>
 the new approximation  agrees with <m>\pi</m> to eight decimal places.
</li>
</ul>
Beware that (E3b), namely <m>\cA=A(h/2)+K\big(\tfrac{h}{2}\big)^k</m>, is saying that <m>K\big(\frac{h}{2}\big)^k</m> is (approximately) the error in <m>A(h/2)</m>, <em>not</em> the error in <m>\cA</m>. You cannot get an <q>even more improved</q> approximation by using
(E4a) to compute <m>K</m> and then adding <m>K\big(\frac{h}{2}\big)^k</m> to the <q>new improved</q>
<m>\cA</m> of (E4b) <mdash/>  doing so just gives <m>\cA+K\big(\tfrac{h}{2}\big)^k</m>, not a more accurate <m>\cA</m>.
</p>
</example>

<example xml:id="eg_SimpsonErrB"><title>Example<nbsp/><xref ref="eg_SimpsonErr"/> revisited</title>
<p>
Suppose again that  we wish to use Simpson's rule (<xref ref="eq_SIMPrule"/>) to evaluate
<m>\int_0^1 e^{-x^2}\,\dee{x}</m> to within an accuracy of <m>10^{-6}</m>, but that we
do not need the degree of certainty provided by Example<nbsp/><xref ref="eg_SimpsonErr"/>.
Observe that we need (approximately) that <m>|K|h^4 \lt 10^{-6}</m>, so if we can
estimate <m>K</m> (using our Richardson trick) then we can estimate the required <m>h</m>.
A commonly used strategy, based on this observation, is to
<ul>
<li>
first apply Simpson's rule twice with some relatively small number of steps and
</li><li>
then use (E4a), with <m>k=4</m>, to estimate <m>K</m> and
</li><li>
then use the condition <m>|K| h^k\le 10^{-6}</m> to determine, approximately,
the number of steps required
</li><li>
and finally apply Simpson's rule with the number of steps just determined.
</li>
</ul>
Let's implement this strategy. First we estimate <m>K</m> by applying Simpson's rule with step sizes
<m>\tfrac{1}{4}</m> and <m>\tfrac{1}{8}</m>. Writing <m>\tfrac{1}{4}=h'</m>, we get
<me>
A(h')=0.74685538  % 0.746855379790987
\qquad
A(h'/2)=0.74682612 %0.746826120527467
</me>
so that (E4a), with <m>k=4</m> and <m>h</m> replaced
by <m>h'</m>, yields
<me>
K=\frac{0.74682612 - 0.74685538}{[1-2^{-4}](1/4)^4}
%=-\frac{0.000031211}{(1/4)^4}
=-7.990\times 10^{-3}
</me>
We want to use a step size <m>h</m> obeying
<me>
|K|h^4\le 10^{-6}
\iff 7.990\times 10^{-3} h^4\le 10^{-6}
\iff h \le\root{4}\of{\frac{1}{7990}} =\frac{1}{9.45}
</me>
like, for example, <m>h=\tfrac{1}{10}</m>. Applying Simpson's rule with
<m>h=\tfrac{1}{10}</m> gives
<me>
A(1/10) = 0.74682495
</me>
The exact answer, to eight decimal places, is <m>0.74682413</m> so the error
in <m>A(1/10)</m> is indeed just under <m>10^{-6}</m>.
</p>

<p>
Suppose now that we change our minds. We want an accuracy of <m>10^{-12}</m>, rather than <m>10^{-6}</m>. We have already estimated <m>K</m>. So now we want to use a 
step size <m>h</m> obeying
<md>
<mrow>
|K|h^4\le 10^{-12}
&amp;\iff 7.99\times 10^{-3} h^4\le 10^{-12}
</mrow><mrow>
&amp;\iff h \le\root{4}\of{\frac{1}{7.99\times 10^9}} =\frac{1}{299.0}
</mrow>
</md>
like, for example, <m>h=\tfrac{1}{300}</m>. Applying Simpson's rule with
<m>h=\tfrac{1}{300}</m> gives, to fourteen decimal places,
<me>
A(1/300) = 0.74682413281344
</me>
The exact answer, to fourteen decimal places, is <m>0.74682413281243</m> so the error
in <m>A(1/300)</m> is indeed just over <m>10^{-12}</m>.
</p>
</example>
</section>

<section xml:id="ap_Romberg">
  <title>Romberg Integration</title>
<p>
The formulae (E4a,b) for <m>K</m> and <m>\cA</m> are,
of course, only<fn><q>Only</q> is a bit strong. Don't underestimate the power
of a good approximation (pun intended).</fn>  approximate since
they are based on (E2), which is an approximation
to (E1). Let's repeat the derivation that leads to
(E4), but using the full (E1),
<me>
\cA=A(h)+Kh^k+K_1h^{k+1}+K_2h^{k+2}+\cdots
</me>
Once again, suppose that we have chosen some <m>h</m> and that we have
evaluated <m>A(h)</m> and <m>A(h/2)</m>. They obey
<md>
<mrow>
\cA&amp;=A(h)+Kh^k+K_1h^{k+1}+K_2h^{k+2}+\cdots \tag{E5a}
</mrow><mrow>
\cA&amp;=A(h/2)+K\big(\tfrac{h}{2}\big)^k
        +K_1\big(\tfrac{h}{2}\big)^{k+1}
        +K_2\big(\tfrac{h}{2}\big)^{k+2}+\cdots \tag{E5b}
</mrow>
</md>
Now, as we did in the derivation of (E4b), multiply
(E5b) by <m>2^k</m> and then
subtract (E5a). This gives
<me>
\left(2^k-1\right)\cA=2^kA(h/2)-A(h)-\half K_1 h^{k+1}-\tfrac{3}{4} K_2 h^{k+1}+\cdots
</me>
and then, dividing across by <m>\left(2^k-1\right)</m>,
<me>
\cA=\frac{2^kA(h/2)-A(h)}{2^k-1}-\frac{1/2}{2^k-1} K_1 h^{k+1}-\frac{3/4}{2^k-1} K_2 h^{k+2}+\cdots
</me>
Hence if we define our <q>new improved approximation</q>
<me>
B(h)=\frac{2^kA(h/2)-A(h)}{2^k-1}\ \text{and}\ 
\tilde K = -\frac{1/2}{2^k-1}K_1\ \text{and}\ 
\tilde K_1 = -\frac{3/4}{2^k-1}K_2
\tag{E6}</me>
we have
<me>
\cA=B(h)+\tilde K h^{k+1}+\tilde K_1 h^{k+2}+\cdots
</me>
which says that <m>B(h)</m> is an approximation to <m>\cA</m> whose error is of order <m>k+1</m>,
one better<fn>That is, the error decays as <m>h^{k+1}</m> as opposed to <m>h^k</m>
<mdash/> so, as <m>h</m> decreases, it gets smaller faster.</fn> than <m>A(h)</m>'s.
</p>

<p>
If <m>A(h)</m> has been computed for three values of <m>h</m>, we can generate <m>B(h)</m>
for two values of <m>h</m> and repeat the above procedure with a new value of
<m>k</m>. And so on.
One widely used numerical integration algorithm,
called Romberg integration<fn>Romberg Integration
was introduced by the German Werner Romberg (1909<ndash/>2003) in
1955.</fn>, applies this procedure repeatedly to the
trapezoidal rule. It is known that the trapezoidal rule approximation
<m>T(h)</m> to an integral <m>I</m> has error behaviour (assuming that the integrand
<m>f(x)</m> is smooth)
<me>
I=T(h)+K_1h^2+K_2h^4+K_3h^6+\cdots
</me>
Only even powers of <m>h</m> appear.
Hence
<md>
<mrow>
T(h)&amp;                         &amp;&amp;\text{ has error of order 2}
</mrow>
<intertext>
so that, using (E6) with <m>k=2</m>,
</intertext>
<mrow>
T_1(h)&amp;=\tfrac{4T(h/2)-T(h)}{3}&amp;&amp;\text{ has error of order 4}
</mrow>
<intertext>
so that, using (E6) with <m>k=4</m>,
</intertext>
<mrow>
T_2(h)&amp;=\tfrac{16T_1(h/2)-T_1(h)}{15}&amp;&amp;\text{ has error of order 6}
</mrow>
<intertext> 
so that, using (E6) with <m>k=6</m>,
</intertext>
<mrow>
T_3(h)&amp;=\tfrac{64T_2(h/2)-T_2(h)}{63}&amp;&amp;\text{ has error of order 8}
</mrow>
</md>
and so on.
We know another method which produces an error of order 4 <mdash/> Simpson's
rule. In fact, <m>T_1(h)</m> is exactly Simpson's rule (for step size
<m>\frac{h}{2}</m>).
</p>
<fact xml:id="eq_Romberg"><title>Romberg integration</title>
<statement><p>
Let <m>T(h)</m> be the trapezoidal rule approximation, with step size <m>h</m>, 
to an integral <m>I</m>. The Romberg integration algorithm is 
<md>
<mrow>
T_1(h)&amp;=\tfrac{4T(h/2)-T(h)}{3} 
</mrow><mrow>
T_2(h)&amp;=\tfrac{16T_1(h/2)-T_1(h)}{15} 
</mrow><mrow>
T_3(h)&amp;=\tfrac{64T_2(h/2)-T_2(h)}{63} 
</mrow><mrow>
&amp;\ \  \vdots \\
T_k(h)&amp;=\tfrac{2^{2k} T_{k-1}(h/2)-T_{k-1}(h)}{2^{2k}-1} 
</mrow><mrow>
&amp;\ \  \vdots
</mrow>
</md>
</p>
</statement>
</fact>


<example xml:id="eg_Romberg3"><title>Finding <m>\pi</m> by Romberg integration</title>
<p>
The following table<fn>The second column, for example,
of the table only reports <m>5</m> decimal places for <m>T(h)</m>.
But many more decimal places of <m>T(h)</m> were used in the computations
of <m>T_1(h)</m> etc.</fn>
illustrates Romberg integration by applying it to
the area <m>A</m> of the integral <m>A=\int_0^1\frac{4}{1+x^2}\dee{x}</m>.
The exact value of this integral is <m>\pi</m> which is <m>3.14159265358979</m>,
to fourteen decimal places.
</p>

  <sidebyside>
   <tabular halign="center" bottom="minor" left="minor" right="minor"  
                                                          top="minor">
   <col />  <col />  <col />  <col /> <col />
     <row>
        <cell><m>h</m></cell>
        <cell><m>T(h)</m></cell>
        <cell><m>T_1(h)</m></cell>
        <cell><m>T_2(h)</m></cell>
        <cell> <m>T_3(h)</m></cell>
     </row> <row>
        <cell>1/4</cell>
        <cell>3.13118</cell>
        <cell>3.14159250246</cell>
        <cell>3.14159266114</cell>
        <cell>3.14159265359003</cell>
     </row> <row>
        <cell>1/8</cell>
        <cell>3.13899</cell>
        <cell>3.141592651225</cell>
        <cell>3.141592653708</cell>
        <cell bottom="none" right="none"></cell>
     </row> <row>
        <cell>1/16</cell>
        <cell>3.14094</cell>
        <cell>3.141592653553</cell>
        <cell bottom="none" right="none"></cell>
        <cell bottom="none" right="none"></cell>
     </row> <row>
        <cell>1/32</cell>
        <cell>3.14143</cell>
        <cell bottom="none" right="none"></cell>
        <cell bottom="none" right="none"></cell>
        <cell bottom="none" right="none"></cell>
      </row>
   </tabular>
   </sidebyside>
<p>
This computation required the evaluation of <m>f(x)=\frac{4}{1+x^2}</m> only
for <m>x=\frac{n}{32}</m> with <m>0\le n\le 32</m> <mdash/> that is, a total
of <m>33</m> evaluations of <m>f</m>. Those <m>33</m> evaluations gave us <m>12</m>
correct decimal places. By way of comparison, <m>T\big(\frac{1}{32}\big)</m>
used the same <m>33</m> evaluations of <m>f</m>, but only gave us <m>3</m> correct
decimal places.
</p>
</example>

<p>
As we have seen, Richardson extrapolation can be used to choose the 
step size so as to achieve some desired degree of accuracy.
We are next going to consider a family of algorithms that extend this idea
to use small step sizes in the part of the domain of integration where it 
is hard to get good accuracy and large step sizes in the part of the domain 
of integration where it is easy to get good accuracy. We will illustrate 
the ideas by applying them to the integral <m>\int_0^1 \sqrt{x}\ \dee{x}</m>.
The integrand <m>\sqrt{x}</m> changes very quickly when <m>x</m> is small and changes slowly when 
<m>x</m> is large. So we will make the step size small near <m>x=0</m> and make the step size large near <m>x=1</m>. 
</p>

<sidebyside width="45%">
<image source="text/figs/squareRoot"/>
</sidebyside>

</section>

<section xml:id="ap_adaptive">
  <title>Adaptive Quadrature</title>
<p>
Richardson extrapolation is also used to choose the step size
so as to achieve some desired degree of accuracy. <q>Adaptive quadrature</q>
refers to a family of algorithms that use small step sizes
in the part of the domain of integration where it is hard to get good
accuracy and large step sizes in the part of the domain of integration
where it is easy to get good accuracy.
</p>

<p>
We'll illustrate the idea using Simpson's rule applied to the
integral <m>\int_a^b f(x)\ \dee{x}</m>, and assuming that we want the error
to be no more than (approximately) some fixed constant  <m>\varepsilon</m>. For example, 
<m>\varepsilon</m> could be <m>10^{-6}</m>. Denote by <m>S(a',b'\,;\,h')</m>,
the answer given when Simpson's rule is applied to the integral
<m>\int_{a'}^{b'} f(x)\ \dee{x}</m> with step size <m>h'</m>.
<ul>
<li> <em>Step 1.</em> 
We start by applying Simpson's rule, combined
with Richardson extrapolation so as to get an error estimate,
with the largest possible step size <m>h</m>. Namely, set <m>h=\tfrac{b-a}{2}</m> and compute
<me>
f(a)\quad
f\big(a+\tfrac{h}{2}\big)\quad
f(a+h)=f\big(\tfrac{a+b}{2}\big)\quad
f\big(a+\tfrac{3h}{2}\big)\quad
f(a+2h)=f(b)
</me>
Then
<me>
S\big(a,b\,;h\big)
=\tfrac{h}{3}\big\{f(a)+4 f\big(a+h\big)+f(b)\big\}
</me>
and
<md>
<mrow>
S\big(a,b\,;\tfrac{h}{2}\big)
&amp;=\tfrac{h}{6}\big\{f(a)+4 f\big(a+\tfrac{h}{2}\big)
                            +2f\big(\tfrac{a+b}{2}\big)
                            +4 f\big(a+\tfrac{3h}{2}\big)
                           +f(b)\big\}
</mrow><mrow>
&amp;=S\big(a,\tfrac{a+b}{2}\,;\tfrac{h}{2}\big)
+S\big(\tfrac{a+b}{2},b\,;\tfrac{h}{2}\big)
</mrow>
</md>
with
<md>
<mrow>
S\big(a,\tfrac{a+b}{2}\,;\,\tfrac{h}{2}\big)
&amp;=\tfrac{h}{6}\big\{f(a)+4 f\big(a+\tfrac{h}{2}\big)
           +f\big(\tfrac{a+b}{2}\big)\big\} 
</mrow><mrow>
S\big(\tfrac{a+b}{2},b\,;\,\tfrac{h}{2}\big)
&amp;=\tfrac{h}{6}\big\{f\big(\tfrac{a+b}{2}\big)+4 f\big(a+\tfrac{3h}{2}\big)
           +f(b)\big\}
</mrow>
</md>
Using the Richardson extrapolation formula (E4a)
with <m>k=4</m> gives that the error in <m>S\big(a,b\,;\,\tfrac{h}{2}\big)</m>
is (approximately)
<me>
\big|K\big(\tfrac{h}{2}\big)^4\big|=\tfrac{1}{15}\Big|
             S\big(a,b\,;\,\tfrac{h}{2}\big)
            -S\big(a,b\,;\,h\big)\Big|
\tag{E7}</me>
If this is smaller than <m>\varepsilon</m>, we have (approximately) the desired
accuracy and stop<fn>It is very common to build in a bit of a safety
margin and require that, for example, <m>\big|K\big(\tfrac{h}{2}\big)^4\big|</m>
be smaller than <m>\tfrac{\varepsilon}{2}</m> rather than <m>\varepsilon</m>.</fn>.
</li><li>
<p>
<em>Step 2.</em> If (E7) is larger than <m>\varepsilon</m>,
we divide the original integral <m>I=\int_a^b f(x)\,\dee{x}</m> into two <q>half-sized</q>
integrals, <m>I_1=\int_a^{\tfrac{a+b}{2}} f(x)\,\dee{x}</m> and
 <m>I_2=\int_{\tfrac{a+b}{2}}^b f(x)\,\dee{x}</m> and repeat the procedure of Step 1
on each of them, but with <m>h</m> replaced by <m>\tfrac{h}{2}</m> and
<m>\varepsilon</m> replaced by <m>\tfrac{\varepsilon}{2}</m> <mdash/> if we can find an approximation,
<m>\tilde I_1</m>, to <m>I_1</m> with an error less than <m>\tfrac{\varepsilon}{2}</m> and an
approximation, <m>\tilde I_2</m>, to <m>I_2</m> with an error less than
<m>\tfrac{\varepsilon}{2}</m>, then <m>\tilde I_1+\tilde I_2</m>
approximates <m>I</m> with an error less than <m>\varepsilon</m>.
 Here is more detail.
<ul>
<li> If the error in the approximation <m>\tilde I_1</m> to <m>I_1</m> and
      the error in the approximation <m>\tilde I_2</m> to <m>I_2</m> are both
      acceptable, then we use <m>\tilde I_1</m> as our final approximation to <m>I_1</m>
      and we use <m>\tilde I_2</m> as our final approximation to <m>I_2</m>.
</li><li>
      If the error in the approximation <m>\tilde I_1</m> to <m>I_1</m> is acceptable
      but the error in the approximation <m>\tilde I_2</m> to <m>I_2</m> is not
      acceptable, then we use <m>\tilde I_1</m> as our final approximation to <m>I_1</m>
      but we subdivide the integral <m>I_2</m>.
</li><li>
      If the error in the approximation <m>\tilde I_1</m> to <m>I_1</m> is not acceptable
      but the error in the approximation <m>\tilde I_2</m> to <m>I_2</m> is 
      acceptable, then we use <m>\tilde I_2</m> as our final approximation to <m>I_2</m>
      but we subdivide the integral <m>I_1</m>.
</li><li>
      If the error in the approximation <m>\tilde I_1</m> to <m>I_1</m> 
      and the error in the approximation <m>\tilde I_2</m> to <m>I_2</m> are both not 
      acceptable, then we subdivide both of the integrals <m>I_1</m> and <m>I_2</m>.
</li>
</ul>
So we <em>adapt</em> the step size as we go.
</p>
</li><li> <em>Steps 3, 4, 5, <m>\cdots</m></em> 
Repeat as required.
</li>
</ul>
</p>

<example xml:id="eg_adaptiveQuad"><title>Adaptive quadrature</title>
<p>
Let's apply adaptive quadrature using Simpson's rule as above with the
goal of computing <m>\int _0^1\sqrt{x}\ \dee{x}</m> with an error of at most
<m>\varepsilon=0.0005=5\times 10^{-4}</m>. Observe that <m>\diff{}{x}\sqrt{x}=\frac{1}{2\sqrt{x}}</m> blows up as <m>x</m> tends to zero. The integrand changes very quickly when <m>x</m> is small. So we will probably need to make the step size small near the limit of integration <m>x=0</m>. 
<ul>
<li>
 <em>Step 1 <mdash/> the interval <m>[0,1]</m>.</em> (The notation <m>[0,1]</m> stands
for the interval <m>0\le x\le 1</m>.)
<md>
<mrow>
S(0,1\,;\tfrac{1}{2})&amp;= 0.63807119 
</mrow><mrow>
S(0,\tfrac{1}{2}\,;\tfrac{1}{4})&amp;= 0.22559223 
</mrow><mrow>
S(\tfrac{1}{2},1\,;\tfrac{1}{4})&amp;= 0.43093403  
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(0,\tfrac{1}{2}\,;\tfrac{1}{4})
                              +S(\tfrac{1}{2},1\,;\tfrac{1}{4})
                              -S(0,1\,;\tfrac{1}{2})\right|
             =0.0012 
</mrow><mrow>
&amp;\gt\varepsilon =0.0005
</mrow>
</md>
This is unacceptably large, so we subdivide the interval <m>[0,1]</m> into
the two halves <m>\big[0,\tfrac{1}{2}\big]</m> and
<m>\big[\tfrac{1}{2},1\big]</m> and apply
the procedure separately to each half.
</li><li>
<em>Step 2a <mdash/> the interval <m>[0,\half]</m>.</em>
<md>
<mrow>
S(0,\tfrac{1}{2}\,;\tfrac{1}{4})&amp;= 0.22559223 
</mrow><mrow>
S(0,\tfrac{1}{4}\,;\tfrac{1}{8})&amp;= 0.07975890 
</mrow><mrow>
S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{8})&amp;= 0.15235819 
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(0,\tfrac{1}{4}\,;\tfrac{1}{8})
                              +S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{8})
                              -S(0,\tfrac{1}{2}\,;\tfrac{1}{4})\right|
             = 0.00043 
</mrow><mrow>
&amp; \gt\tfrac{\varepsilon}{2} = 0.00025
</mrow>
</md>
This error is unacceptably large.
</li><li>
<em>Step 2b <mdash/> the interval <m>[\tfrac{1}{2},1]</m>.</em>
<md>
<mrow>
S(\tfrac{1}{2},1\,;\tfrac{1}{4})&amp;= 0.43093403 
</mrow><mrow>
S(\tfrac{1}{2},\tfrac{3}{4}\,;\tfrac{1}{8})&amp;= 0.19730874 
</mrow><mrow>
S(\tfrac{3}{4},1\,;\tfrac{1}{8})&amp;= 0.23365345  
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(\tfrac{1}{2},\tfrac{3}{4}\,;\tfrac{1}{8})
                              +S(\tfrac{3}{4},1\,;\tfrac{1}{8})
                              -S(\tfrac{1}{2},1\,;\tfrac{1}{4})\right|
             = 0.0000019 
</mrow><mrow>
&amp;\lt\tfrac{\varepsilon}{2} = 0.00025
</mrow>
</md>
This error is acceptable.
</li><li>
<p>
<em>Step 2 resum&#xE9;.</em> 
The error for the interval <m>[\tfrac{1}{2},1]</m> is small enough, so we accept
<me>
S(\tfrac{1}{2},1\,;\tfrac{1}{8})
  = S(\tfrac{1}{2},\tfrac{3}{4}\,;\tfrac{1}{8})
   + S(\tfrac{3}{4},1\,;\tfrac{1}{8})
  = 0.43096219
</me>
as the approximate value of <m>\int_{1/2}^1\sqrt{x}\,\dee{x}</m>.
</p>
<p> The error for the interval <m>[0,\tfrac{1}{2}]</m> is
unacceptably large, so we subdivide the interval <m>[0,\tfrac{1}{2}]</m> into
the two halves <m>[0,\tfrac{1}{4}]</m> and <m>[\tfrac{1}{4},\tfrac{1}{2}]</m>
and apply the procedure separately to each half. 
</p>
</li><li>
<em>Step 3a <mdash/> the interval <m>[0,\tfrac{1}{4}]</m>.</em>
<md>
<mrow>
S(0,\tfrac{1}{4}\,;\tfrac{1}{8})&amp;= 0.07975890  
</mrow><mrow>
S(0,\tfrac{1}{8}\,;\tfrac{1}{16})&amp;= 0.02819903   
</mrow><mrow>
S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{16})&amp;= 0.05386675   
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(0,\tfrac{1}{8}\,;\tfrac{1}{16})
                              +S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{16})
                              -S(0,\tfrac{1}{4}\,;\tfrac{1}{8})\right|
</mrow><mrow>
            &amp;= 0.000153792 > \tfrac{\varepsilon}{4} = 0.000125
</mrow>
</md>
This error is unacceptably large.
</li><li>
<em>Step 3b <mdash/> the interval <m>[\tfrac{1}{4},\tfrac{1}{2}]</m>.</em>
<md>
<mrow>
S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{8})&amp;= 0.15235819   
</mrow><mrow>
S(\tfrac{1}{4},\tfrac{3}{8}\,;\tfrac{1}{16})&amp;= 0.06975918  
</mrow><mrow>
S(\tfrac{3}{8},\tfrac{1}{2}\,;\tfrac{1}{16})&amp;= 0.08260897   
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(\tfrac{1}{4},\tfrac{3}{8}\,;\tfrac{1}{16})
                              +S(\tfrac{3}{8},\tfrac{1}{2}\,;\tfrac{1}{16})
                              -S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{8})\right|
</mrow><mrow>
            &amp; =  0.00000066  \lt\tfrac{\varepsilon}{4} = 0.000125
</mrow>
</md>
This error is acceptable.
</li><li>
<p>
<em>Step 3 resum&#xE9;.</em> 
The error for the interval
<m>[\tfrac{1}{4},\tfrac{1}{2}]</m> is small enough, so we accept
<me>
S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{16})
  = S(\tfrac{1}{4},\tfrac{3}{8}\,;\tfrac{1}{16})
   + S(\tfrac{3}{8},\tfrac{1}{2}\,;\tfrac{1}{16})
  = 0.15236814
</me>
as the approximate value of <m>\int_{1/4}^{1/2}\sqrt{x}\,\dee{x}</m>.
</p>

<p>
The error for the interval <m>[0,\tfrac{1}{4}]</m> is
unacceptably large, so we subdivide the interval <m>[0,\tfrac{1}{4}]</m> into
the two halves <m>[0,\tfrac{1}{8}]</m> and <m>[\tfrac{1}{8},\tfrac{1}{4}]</m>
and apply the procedure separately to each half. 
</p>
</li><li>
<em>Step 4a <mdash/> the interval <m>[0,\tfrac{1}{8}]</m>.</em>
<md>
<mrow>
S(0,\tfrac{1}{8}\,;\tfrac{1}{16})&amp;= 0.02819903   
</mrow><mrow>
S(0,\tfrac{1}{16}\,;\tfrac{1}{32})&amp;= 0.00996986    
</mrow><mrow>
S(\tfrac{1}{16},\tfrac{1}{8}\,;\tfrac{1}{32})&amp;= 0.01904477    
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(0,\tfrac{1}{16}\,;\tfrac{1}{32})
                              +S(\tfrac{1}{16},\tfrac{1}{8}\,;\tfrac{1}{32})
                              -S(0,\tfrac{1}{8}\,;\tfrac{1}{16})\right|
</mrow><mrow>
            &amp; =  0.000054  \lt \tfrac{\varepsilon}{8} = 0.0000625
</mrow>
</md>
This error is acceptable.
</li><li>
<em>Step 4b <mdash/> the interval <m>[\tfrac{1}{8},\tfrac{1}{4}]</m>.</em>
<md>
<mrow>
S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{16})&amp;= 0.05386675   
</mrow><mrow>
S(\tfrac{1}{8},\tfrac{3}{16}\,;\tfrac{1}{32})&amp;= 0.02466359  
</mrow><mrow>
S(\tfrac{3}{16},\tfrac{1}{4}\,;\tfrac{1}{32})&amp;= 0.02920668   
</mrow><mrow>
\text{error}&amp;=\tfrac{1}{15}\left|S(\tfrac{1}{8},\tfrac{3}{16}\,;\tfrac{1}{32})
                              +S(\tfrac{3}{6},\tfrac{1}{4}\,;\tfrac{1}{32})
                              -S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{16})\right|
</mrow><mrow>
            &amp; =  0.00000024  \lt \tfrac{\varepsilon}{8} = 0.0000625
</mrow>
</md>
This error is acceptable.
</li><li>
<p>
<em>Step 4 resum&#xE9;.</em> The error for the interval
<m>[0,\tfrac{1}{8}]</m> is small enough, so we accept
<me>
S(0,\tfrac{1}{8}\,;\tfrac{1}{32})
  = S(0,\tfrac{1}{16}\,;\tfrac{1}{32})
   + S(\tfrac{1}{16},\tfrac{1}{8}\,;\tfrac{1}{32})
  =  0.02901464
</me>
as the approximate value of <m>\int_0^{1/8}\sqrt{x}\,\dee{x}</m>.
</p>
<p>
The error for the interval
<m>[\tfrac{1}{8},\tfrac{1}{4}]</m> is small enough, so we accept
<me>
S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{32})
  = S(\tfrac{1}{8},\tfrac{3}{16}\,;\tfrac{1}{32})
   + S(\tfrac{3}{16},\tfrac{1}{4}\,;\tfrac{1}{32})
  = 0.05387027
</me>
as the approximate value of <m>\int_{1/8}^{1/4}\sqrt{x}\,\dee{x}</m>.
</p>
</li><li>
<em>Conclusion.</em> The approximate value for <m>\int_0^1\sqrt{x}\ \dee{x}</m>
is
<md>
<mrow>
&amp; S(0,\tfrac{1}{8}\,;\tfrac{1}{32})
+S(\tfrac{1}{8},\tfrac{1}{4}\,;\tfrac{1}{32})
+S(\tfrac{1}{4},\tfrac{1}{2}\,;\tfrac{1}{16})
+S(\tfrac{1}{2},1\,;\tfrac{1}{8})
</mrow><mrow>
&amp;\hskip1in =0.66621525
\tag{E8}
</mrow>
</md>
</li>
</ul>
Of course the exact value of <m>\int_0^1\sqrt{x}\ \dee{x}=\tfrac{2}{3}</m>,
so the actual error in our approximation is
<me>
\tfrac{2}{3}-0.66621525 = 0.00045 \lt \varepsilon = 0.0005
</me>
Here is what Simpson's rule gives us when applied with some fixed step sizes. 
<md>
<mrow>
S(0,1\,;\tfrac{1}{8})  &amp;= 0.66307928 
</mrow><mrow>
S(0,1\,;\tfrac{1}{16}) &amp;= 0.66539819 
</mrow><mrow>
S(0,1\,;\tfrac{1}{32}) &amp;= 0.66621818 
</mrow><mrow>
S(0,1\,;\tfrac{1}{64}) &amp;= 0.66650810
</mrow>
</md> 
So to get an error comparable to that in (E8) from Simpson's rule with
a fixed step size, we need to use <m>h=\frac{1}{32}</m>. In (E8) the step size
<m>h=\frac{1}{32}</m> was just used on the subinterval <m>\big[0,\frac{1}{4}\big]</m>.
</p>
</example>

</section>



</appendix>
